TODO:

Priority
	PBRT
	progressive rendering/sampling
	general optimizations

Images
	texture types (float)
	texture handling/filtering
	transparency
	HDR and tone-mapping
	environment map / custom sky definitions

Features
	progressive rendering - non-random sampling; render in epochs and blend together
	importance sampling
	first-hit rasterization
	GPU compute/RTX path?
	break out scene definitions into it's own files; prep for model/scene loading

Optimizations
	General
		use object IDs and ranges instead of pointers (object cache)
			don't calculate position + normal for each nearer object, instead store object ID and calculate after we have found the nearest object.

		push transformations to leaf objects, share identical transformations (transform cache)

		SIMD
			aabb_lane
			looking into ray_lane again (didn't work first time, too divergent?? could have been buggy)

Performance: 640x480x128 - Random Scene - Debug-Optimized Build
	
	Base Scalar  : 107s		{48s Release}
	SIMD v3      : 81.5s 	{29s Release}
	Inlining     : 29s		{29s}
	__vectorcall : 29s 		{29s}
	SIMD Spheres : 13s
	XorShift Rand: 11s 
	BVH SIMD 	 : 5.5s

Performance: 640x480x256 - Random 16x Scene - Release Build

	BVH SIMD 	: 32.4s, 31.2s, 31.8s
	LIST SIMD 	: 39.6s, 40s			{128x: 19s}

	New minmax  	 : 12.4s, 11.3s			{10s with clang??}
	Iterative Compute: 10.7s, 11.3s 		{not really any change}
	Iterative BVH 	 : 8.5s 				{only on release build}

Notes:
	https://bitshifter.github.io/2018/06/04/simd-path-tracing/
	http://aras-p.info/blog/2018/04/13/Daily-Pathtracer-9-A-wild-ryg-appears/

progressive rendering:
	This is somewhat non-trivial regarding thread scheduling -
	easiest way to avoid load balancing problems would be to spawn a
	task for each sample that renders the whole image at 1x and 
	accumulate these into a float buffer.
	This isn't too hard to implement - just have to spawn all tasks
	and add oGL code for rendering the in-progress float buffer.
	However, note that we can't have many threads writing to the same
	portion of the image at the same time, so would need to synchronize
	(slow) or have a separate buffer for each thread, then composite them
	together.
	For now this isn't really necessary as we can just render the image with
	low samples during scene development, though an "add samples" option would
	be useful.
	So I'll do this when I feel like it.

